{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDiguRWE7zt8sgHqmKgvg3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaf-ys/Forward-and-Backward-Propagation-in-MLP/blob/main/Forward%26Backward_Propagation_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 定义一个简单的MLP模型\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # 第一层到第二层的线性变换，输入维度为2，输出维度为1，包含偏置\n",
        "        self.layer0 = nn.Linear(2, 1)\n",
        "        # 第二层到第三层的线性变换，输入维度为1，输出维度为1，包含偏置\n",
        "        self.layer1 = nn.Linear(1, 1)\n",
        "        # Sigmoid激活函数\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 通过第一层\n",
        "        x = self.layer0(x)\n",
        "        x = self.activation(x)\n",
        "        # 通过第二层\n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "# 创建模型实例\n",
        "model = SimpleMLP()\n",
        "\n",
        "# 初始化权重和偏置\n",
        "with torch.no_grad():\n",
        "    model.layer0.weight.fill_(0.5)\n",
        "    model.layer0.bias.fill_(1.0)\n",
        "    model.layer1.weight.fill_(0.5)\n",
        "    model.layer1.bias.fill_(1.0)\n",
        "\n",
        "# 输入数据\n",
        "input = torch.tensor([1.0, 2.0])\n",
        "\n",
        "# 进行一次前向传播\n",
        "output = model(input)\n",
        "print(\"Output of the MLP:\", output.item())\n",
        "\n",
        "target = torch.tensor([0.4])\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# 前向传播\n",
        "output = model(input)\n",
        "loss = criterion(output, target)\n",
        "\n",
        "# 反向传播和优化\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "# 反向传播后查看梯度\n",
        "print(\"\\nGradients after backward:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name} grad: {param.grad}\")\n",
        "\n",
        "# 输出更新后的权重和偏置\n",
        "print(\"\\nUpdated weights and biases:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cHIXsc-fMUK",
        "outputId": "96d21fcf-be45-4a09-a503-162aa97d5943"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the MLP: 0.8118491768836975\n",
            "\n",
            "Gradients after backward:\n",
            "layer0.weight grad: tensor([[0.0044, 0.0088]])\n",
            "layer0.bias grad: tensor([0.0044])\n",
            "layer1.weight grad: tensor([[0.1163]])\n",
            "layer1.bias grad: tensor([0.1258])\n",
            "\n",
            "Updated weights and biases:\n",
            "layer0.weight tensor([[0.4996, 0.4991]])\n",
            "layer0.bias tensor([0.9996])\n",
            "layer1.weight tensor([[0.4884]])\n",
            "layer1.bias tensor([0.9874])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# 初始化权重和偏置\n",
        "W0 = np.array([[0.5, 0.5]])  # 权重从输入到隐藏层\n",
        "b0 = np.array([1.0])         # 偏置从输入到隐藏层\n",
        "W1 = np.array([[0.5]])       # 权重从隐藏到输出层\n",
        "b1 = np.array([1.0])         # 偏置从隐藏到输出层\n",
        "\n",
        "# 输入数据\n",
        "input_data = np.array([1.0, 2.0])\n",
        "\n",
        "# 目标数据\n",
        "target = np.array([0.4])\n",
        "\n",
        "# 学习率\n",
        "lr = 0.1\n",
        "\n",
        "# 前向传播\n",
        "Z1 = np.dot(W0, input_data) + b0\n",
        "A1 = sigmoid(Z1)\n",
        "Z2 = np.dot(W1, A1) + b1\n",
        "A2 = sigmoid(Z2)\n",
        "\n",
        "# 计算输出层的误差\n",
        "delta_2 = (A2 - target) * sigmoid_derivative(A2)\n",
        "\n",
        "# 反向传播误差到前一层\n",
        "delta_1 = delta_2 * W1 * sigmoid_derivative(A1)\n",
        "\n",
        "# 梯度更新\n",
        "W1 -= lr * delta_2 * A1.T\n",
        "b1 -= lr * delta_2\n",
        "W0 -= lr * np.dot(delta_1, input_data.reshape(1, -1))\n",
        "b0 -= lr * delta_1.squeeze()\n",
        "\n",
        "# 输出计算结果\n",
        "print(\"Updated weights and biases from input to hidden layer:\")\n",
        "print(\"W0:\", W0)\n",
        "print(\"b0:\", b0)\n",
        "print(\"Updated weights and biases from hidden to output layer:\")\n",
        "print(\"W1:\", W1)\n",
        "print(\"b1:\", b1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8mZk9S6A5aX",
        "outputId": "b900d101-49c5-4916-f10c-4931ecb88398"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights and biases from input to hidden layer:\n",
            "W0: [[0.49977949 0.49955898]]\n",
            "b0: [0.99977949]\n",
            "Updated weights and biases from hidden to output layer:\n",
            "W1: [[0.49418622]]\n",
            "b1: [0.993709]\n"
          ]
        }
      ]
    }
  ]
}