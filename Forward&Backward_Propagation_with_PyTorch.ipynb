{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaf-ys/Forward-and-Backward-Propagation-in-MLP/blob/main/Forward%26Backward_Propagation_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple MLP model\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Linear transformation from the first layer to the second layer, input dimension is 2, output dimension is 1, with bias\n",
        "        self.layer0 = nn.Linear(2, 1)\n",
        "        # Linear transformation from the second layer to the third layer, input dimension is 1, output dimension is 1, with bias\n",
        "        self.layer1 = nn.Linear(1, 1)\n",
        "        # Sigmoid activation function\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through the first layer\n",
        "        x = self.layer0(x)\n",
        "        x = self.activation(x)\n",
        "        # Pass through the second layer\n",
        "        x = self.layer1(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "# Create a model instance\n",
        "model = SimpleMLP()\n",
        "\n",
        "# Initialize weights and biases\n",
        "with torch.no_grad():\n",
        "    model.layer0.weight.fill_(0.5)\n",
        "    model.layer0.bias.fill_(1.0)\n",
        "    model.layer1.weight.fill_(0.5)\n",
        "    model.layer1.bias.fill_(1.0)\n",
        "\n",
        "# Input data\n",
        "input = torch.tensor([1.0, 2.0])\n",
        "\n",
        "# Perform a forward pass\n",
        "output = model(input)\n",
        "print(\"Output of the MLP:\", output.item())\n",
        "\n",
        "target = torch.tensor([0.4])\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Forward pass\n",
        "output = model(input)\n",
        "loss = criterion(output, target)\n",
        "\n",
        "# Backward pass and optimization\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "# Check gradients after backward pass\n",
        "print(\"\\nGradients after backward:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name} grad: {param.grad}\")\n",
        "\n",
        "# Output updated weights and biases\n",
        "print(\"\\nUpdated weights and biases:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cHIXsc-fMUK",
        "outputId": "5ff42699-de27-43e9-f17a-4f599a6d28d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the MLP: 0.8118491768836975\n",
            "\n",
            "Gradients after backward:\n",
            "layer0.weight grad: tensor([[0.0044, 0.0088]])\n",
            "layer0.bias grad: tensor([0.0044])\n",
            "layer1.weight grad: tensor([[0.1163]])\n",
            "layer1.bias grad: tensor([0.1258])\n",
            "\n",
            "Updated weights and biases:\n",
            "layer0.weight tensor([[0.4996, 0.4991]])\n",
            "layer0.bias tensor([0.9996])\n",
            "layer1.weight tensor([[0.4884]])\n",
            "layer1.bias tensor([0.9874])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Initialize weights and biases\n",
        "W0 = np.array([[0.5, 0.5]])  # Weights from input to hidden layer\n",
        "b0 = np.array([1.0])         # Biases from input to hidden layer\n",
        "W1 = np.array([[0.5]])       # Weights from hidden to output layer\n",
        "b1 = np.array([1.0])         # Biases from hidden to output layer\n",
        "\n",
        "# Input data\n",
        "input_data = np.array([1.0, 2.0])\n",
        "\n",
        "# Target data\n",
        "target = np.array([0.4])\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Forward propagation\n",
        "Z1 = np.dot(W0, input_data) + b0\n",
        "A1 = sigmoid(Z1)\n",
        "Z2 = np.dot(W1, A1) + b1\n",
        "A2 = sigmoid(Z2)\n",
        "\n",
        "# Compute error at the output layer\n",
        "delta_2 = 2*(A2 - target) * sigmoid_derivative(A2)\n",
        "\n",
        "# Backpropagate the error to the previous layer\n",
        "delta_1 = delta_2 * W1 * sigmoid_derivative(A1)\n",
        "\n",
        "# Update gradients\n",
        "W1 -= lr * delta_2 * A1.T\n",
        "b1 -= lr * delta_2\n",
        "W0 -= lr * np.dot(delta_1, input_data.reshape(1, -1))\n",
        "b0 -= lr * delta_1.squeeze()\n",
        "\n",
        "# Output the results\n",
        "print(\"Updated weights and biases from input to hidden layer:\")\n",
        "print(\"W0:\", W0)\n",
        "print(\"b0:\", b0)\n",
        "print(\"Updated weights and biases from hidden to output layer:\")\n",
        "print(\"W1:\", W1)\n",
        "print(\"b1:\", b1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8mZk9S6A5aX",
        "outputId": "97d6f0c9-060a-4965-bb05-409010b41860"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights and biases from input to hidden layer:\n",
            "W0: [[0.49955898 0.49911796]]\n",
            "b0: [0.99955898]\n",
            "Updated weights and biases from hidden to output layer:\n",
            "W1: [[0.48837245]]\n",
            "b1: [0.987418]\n"
          ]
        }
      ]
    }
  ]
}